from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector, SearchType

from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
import numpy as np

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create Groq model
model=Groq(id='qwen-qwq-32b')

# Create Ollama embedder
embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

# ä¿å­˜åŸå§‹æ–¹æ³•
original_get_embedding = embedder.get_embedding

# ä¸´æ—¶ä¿®å¤åµŒå…¥å™¨è¾“å‡º
def fixed_get_embedding(text):
    embedding = original_get_embedding(text)
    return np.array(embedding).flatten().tolist()

embedder.get_embedding = fixed_get_embedding

# Create a knowledge base of PDFs from URLs
#knowledge_base = PDFUrlKnowledgeBase(
#    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
#    vector_db=PgVector(
#        table_name="recipes",
#        db_url=db_url,
#        search_type=SearchType.hybrid,
#        embedder=embedder,
#    ),
#)

# Create a knowledge base of PDFs from local path
pdf_path = "D:/Chrome/Downloads/ThaiRecipes.pdf"

knowledge_base = PDFKnowledgeBase(
    path=pdf_path,
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=embedder,
    ),
)

# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load(recreate=True)

agent = Agent(
    model=model,
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
    markdown=True,
    add_datetime_to_instructions=True,
)

def update_knowledge_base(path):
    global knowledge_base  # ç¡®ä¿ `knowledge_base` å˜é‡åŒæ­¥æ›´æ–°

    # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“å¹¶æ›´æ–° `agent.knowledge`
    new_knowledge_base = PDFKnowledgeBase(
        path=path,
        vector_db=PgVector(
            table_name="recipes",
            db_url=db_url,
            search_type=SearchType.hybrid,
            embedder=embedder,
        ),
    )
    # é‡æ–°åŠ è½½çŸ¥è¯†åº“
    new_knowledge_base.load(recreate=True)

    # æ›´æ–°å…¨å±€ `knowledge_base` å’Œ `agent.knowledge`
    knowledge_base = new_knowledge_base
    agent.knowledge = knowledge_base

    return knowledge_base


# æµ‹è¯•åµŒå…¥å™¨è¾“å‡º
def test_fixed_get_embedding():
    print("æµ‹è¯•åµŒå…¥å™¨è¾“å‡º...")
    test_text = "Test sentence"
    test_embedding = embedder.get_embedding(test_text)  # ä½¿ç”¨ get_embedding
    print(f"åŸå§‹åµŒå…¥: {test_embedding[:5]}")  # æ˜¾ç¤ºå‰5ä¸ªå€¼
    print(f"ç»´åº¦: {len(test_embedding)}")
    if isinstance(test_embedding, list) and all(isinstance(x, (int, float)) for x in test_embedding):
        print("åµŒå…¥æ˜¯ä¸€ç»´åˆ—è¡¨ï¼Œæ ¼å¼æ­£ç¡®")
    else:
        print("åµŒå…¥æ ¼å¼é”™è¯¯ï¼Œå°è¯•å±•å¹³...")
        test_embedding = np.array(test_embedding).flatten().tolist()
        print(f"å±•å¹³ååµŒå…¥: {test_embedding[:5]}, ç»´åº¦: {len(test_embedding)}")


# æä¾›å¤–éƒ¨è°ƒç”¨çš„ RAG é—®ç­”æ¥å£
def query_rag(question):
    """åŸºäº PDF çŸ¥è¯†åº“çš„ RAG æŸ¥è¯¢"""
    return agent.print_response(question, markdown=True,stream = True)

# äº¤äº’æ¨¡å¼ï¼ˆä»…åœ¨ CLI ä¸‹è¿è¡Œï¼‰
def agent_respond():
    print("ğŸ¤– T-AI RAG Agentï¼šè¾“å…¥ 'exit' é€€å‡ºå¯¹è¯ã€‚")
    while True:
        print(f'\n')
        user_input = input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ")
        if user_input.lower() in ["exit", "é€€å‡º"]:
            print("ğŸ”š ç»“æŸå¯¹è¯ã€‚")
            break
        query_rag(user_input)

# æµ‹è¯•æ¨¡å¼
if __name__ == "__main__":
    agent_respond()