import os
from groq import Groq
#from groq_mock import Groq
import json
from ai_agent_omni import execute_ui
from prompt_manager import get_agent_prompt
from tools.tools import tools, google_search, send_email, get_current_time

# Initialize Groq client (using your provided API key)
client = Groq(api_key=os.getenv('GROQ_API_KEY'),timeout=60)

# ç”Ÿæˆæ ¼å¼åŒ–åçš„ JSON å·¥å…·åˆ—è¡¨
tools_json = json.dumps(tools, indent=2, ensure_ascii=False)

available_functions = {
    'google_search': google_search,
    'send_email': send_email,
    'get_current_time': get_current_time,
    'execute_ui': execute_ui
}


def query_groq(messages: list) -> str:
    """Call Groq API to get LLM response for the given conversation messages."""
    while True:
        try:
            response = client.chat.completions.create(
                model='qwen-qwq-32b',  # ä½¿ç”¨ Groq æ”¯æŒçš„æ¨¡å‹
                messages=messages,
                temperature=0.6,
                tools=tools,
                tool_choice="auto"  # Let model decide when to use tools
            )
        except Exception as e:
            print(f"âŒ è°ƒç”¨ Groq API å¤±è´¥: {e}")
            return "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"


        # ç¡®ä¿ response.choices å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if not response.choices or not hasattr(response.choices[0], 'message'):
            print("âŒ Groq API å“åº”æ ¼å¼é”™è¯¯: response.choices ä¸ºç©ºæˆ–ç¼ºå°‘ message")
            return "API å“åº”å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•ã€‚"

        # Extract the response and any tool call responses
        response_message = response.choices[0].message

        # è·å–æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨
        tool_calls = getattr(response_message, 'tool_calls', None)

        output = ''

        if tool_calls:

            # There may be multiple tool calls in the response
            for tool in tool_calls:
                # Ensure the function is available, and then call it
                if function_to_call := available_functions.get(tool.function.name):
                    print('ğŸ”¹Calling function:', tool.function.name)
                    # è§£æ JSON ç»“æ„ï¼Œç¡®ä¿ `arguments` æ˜¯æ ‡å‡†å­—å…¸æ ¼å¼
                    try:
                        # ç¡®ä¿ arguments æ˜¯æ ‡å‡† Python å­—å…¸
                        raw_arguments = tool.function.arguments  # ç›´æ¥ä» tool è¯»å–å‚æ•°

                        if isinstance(raw_arguments, str):
                            print(f"âŒ raw_arguments str")
                            arguments = json.loads(raw_arguments)  # ç¡®ä¿æ˜¯ dict
                        elif isinstance(raw_arguments, dict):
                            print(f"âœ… raw_arguments: dict")
                            arguments = raw_arguments  # å¦‚æœå·²ç»æ˜¯ dictï¼Œç›´æ¥ä½¿ç”¨
                        else:
                            arguments = {}  # å…œåº•é˜²æ­¢ NoneType
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
                        continue  # è·³è¿‡å½“å‰å·¥å…·è°ƒç”¨

                    print('ğŸ“¥Arguments:', arguments)
                    # å¤„ç†æ— å‚æ•°å‡½æ•°
                    if arguments:
                        output = function_to_call(**arguments)


                    print('âœ…Function output:', output)

                    # Add the LLM's response to the conversation
                    messages.append({
                        "role": "assistant",
                        "content": response_message.content,
                        "tool_calls": [
                            {
                                "id": tool.id,
                                "function": {
                                    "name": tool.function.name,
                                    "arguments": tool.function.arguments
                                },
                                "type": "function"
                            }
                        ]
                    })
                    # Add the tool response to the conversation
                    messages.append({'tool_call_id': tool.id, 'role': "tool", 'name': tool.function.name,
                                     'content': json.dumps(output) if isinstance(output, (dict, list)) else str(output)})

                else:
                    print('âŒFunction', tool.function.name, 'not found')

        # Only needed to chat with the model using the tool call results
        if not tool_calls:
            print('No tool calls returned from model')
            assistant_reply = response.choices[0].message.content if hasattr(response_message, 'content') else ""
            assistant_reply = str(assistant_reply)
            break

    return assistant_reply


def agent_respond(messages: list):
    """Continuous conversation mode"""
    while True:
        user_input = input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
        if user_input.lower() in ["exit", "é€€å‡º"]:
            print("ğŸ”š ç»“æŸå¯¹è¯ã€‚")
            break

        messages.append({'role': 'user', 'content': user_input})
        assistant_reply = query_groq(messages)

        if assistant_reply:
            print(f'\nğŸ¤– AI: {assistant_reply}\n')
            messages.append({"role": "assistant", "content": assistant_reply})
        else:
            print("âš ï¸ AI æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„å“åº”ï¼Œè¯·é‡è¯•ã€‚")


if __name__ == "__main__":
    print("ğŸ¤– æ¬¢è¿ä½¿ç”¨ T-AIAgent æ™ºèƒ½åŠ©æ‰‹ï¼è¾“å…¥ 'exit' é€€å‡ºå¯¹è¯ã€‚")

    prompt = get_agent_prompt()

    # ç»´æŠ¤æ•´ä¸ªå¯¹è¯å†å²
    messages = [{'role': 'system', 'content': prompt}]

    # è¿›å…¥å¯¹è¯ä¸»å¾ªç¯
    agent_respond(messages)