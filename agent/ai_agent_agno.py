from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.storage.postgres import PostgresStorage
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector, SearchType

from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.tools.yfinance import YFinanceTools
from tools.email_tool_agno import SendEmailTools
import numpy as np

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
storage = PostgresStorage(table_name="agent_sessions", db_url=db_url)

# Create Groq model
model=Groq(id='qwen-qwq-32b',timeout=60)

# Create Ollama embedder
embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

# Create a knowledge base of PDFs from URLs
#knowledge_base = PDFUrlKnowledgeBase(
#    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
#    vector_db=PgVector(
#        table_name="pdf_url_documents",
#        db_url=db_url,
#        search_type=SearchType.hybrid,
#        embedder=embedder,
#    ),
#)

# Create a knowledge base of PDFs from local path
#pdf_path = "D:/Chrome/Downloads/ThaiRecipes.pdf"

#knowledge_base = PDFKnowledgeBase(
#    path=pdf_path,
#   # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
#    vector_db=PgVector(
#        table_name="pdf_documents",
#        db_url=db_url,
#        search_type=SearchType.hybrid,
#        embedder=embedder,
#    ),
#)

# Create Website Knowledge Base
knowledge_base = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    # Number of links to follow from the seed URLs
    max_links=10,
    #Maximum depth to crawl
    max_depth=5,
    # Table name: website_documents
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=embedder,
    ),
)

agent = Agent(
    model=model,
    tools=[DuckDuckGoTools(), Newspaper4kTools(),SendEmailTools(),
           YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    show_tool_calls=True,
    knowledge=knowledge_base,
    search_knowledge=True,
    storage=storage,
    add_history_to_messages=True,
    read_chat_history=True,
    markdown=True,
    add_datetime_to_instructions=True,
    #session_id='172eabdb-9427-4867-aba9-e8b3bc24da3c',
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load(recreate=True)


def update_knowledge_base(path):
    global knowledge_base  # ç¡®ä¿ `knowledge_base` å˜é‡åŒæ­¥æ›´æ–°

    # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“å¹¶æ›´æ–° `agent.knowledge`
    new_knowledge_base = PDFKnowledgeBase(
        path=path,
        vector_db=PgVector(
            table_name="recipes",
            db_url=db_url,
            search_type=SearchType.hybrid,
            embedder=embedder,
        ),
    )
    # é‡æ–°åŠ è½½çŸ¥è¯†åº“
    new_knowledge_base.load(recreate=True)

    # æ›´æ–°å…¨å±€ `knowledge_base` å’Œ `agent.knowledge`
    knowledge_base = new_knowledge_base
    #agent.knowledge = knowledge_base

    return knowledge_base


# æµ‹è¯•åµŒå…¥å™¨è¾“å‡º
def test_fixed_get_embedding():
    print("æµ‹è¯•åµŒå…¥å™¨è¾“å‡º...")
    test_text = "Test sentence"
    test_embedding = embedder.get_embedding(test_text)  # ä½¿ç”¨ get_embedding
    print(f"åŸå§‹åµŒå…¥: {test_embedding[:5]}")  # æ˜¾ç¤ºå‰5ä¸ªå€¼
    print(f"ç»´åº¦: {len(test_embedding)}")
    if isinstance(test_embedding, list) and all(isinstance(x, (int, float)) for x in test_embedding):
        print("åµŒå…¥æ˜¯ä¸€ç»´åˆ—è¡¨ï¼Œæ ¼å¼æ­£ç¡®")
    else:
        print("åµŒå…¥æ ¼å¼é”™è¯¯ï¼Œå°è¯•å±•å¹³...")
        test_embedding = np.array(test_embedding).flatten().tolist()
        print(f"å±•å¹³ååµŒå…¥: {test_embedding[:5]}, ç»´åº¦: {len(test_embedding)}")


# æä¾›å¤–éƒ¨è°ƒç”¨çš„ RAG é—®ç­”æ¥å£
def query_agent(question, knowledge_base_override=None):
    """åŸºäº PDF çŸ¥è¯†åº“çš„ RAG æŸ¥è¯¢"""

    # å¦‚æœ `app_rag.py` ä¼ å…¥äº†æ–°çš„ `knowledge_base`ï¼Œä½¿ç”¨å®ƒ
    #if knowledge_base_override:
       # agent.knowledge = knowledge_base_override

    try:
        agent.print_response(question, markdown=True, stream=True)
    except Exception as e:
        print(f"âš ï¸ å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
    # è¿”å›ä»…åŒ…å« content çš„ç”Ÿæˆå™¨
    #return (response.content for response in agent.run(question, stream=True))


# äº¤äº’æ¨¡å¼ï¼ˆä»…åœ¨ CLI ä¸‹è¿è¡Œï¼‰
def agent_respond():
    print("ğŸ¤– T-AI RAG Agentï¼šè¾“å…¥ 'exit' é€€å‡ºå¯¹è¯ã€‚")
    while True:
        print(f'\n')
        user_input = input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ")
        if user_input.lower() in ["exit", "é€€å‡º"]:
            print("ğŸ”š ç»“æŸå¯¹è¯ã€‚")
            break
        query_agent(user_input)
        #for chunk in response_stream:
        #    print(chunk, end="", flush=True)  # CLI ä¸‹æµå¼æ‰“å°

# æµ‹è¯•æ¨¡å¼
if __name__ == "__main__":
    agent_respond()