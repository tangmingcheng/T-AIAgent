import ollama
import json

from ollama import ChatResponse, chat
from tools import tools, google_search, send_email


# ç”Ÿæˆæ ¼å¼åŒ–åçš„ JSON å·¥å…·åˆ—è¡¨
tools_json = json.dumps(tools, indent=2, ensure_ascii=False)

available_functions = {
  'google_search': google_search,
  'send_email': send_email,
}


def query_llm(messages: list) -> str:
    """è°ƒç”¨ Ollama æœ¬åœ°APIè·å– LLM å¯¹æŒ‡å®šå¯¹è¯æ¶ˆæ¯çš„å›å¤ã€‚"""

    response =ollama.chat(
        model="llama3.1",
        messages=messages,
        tools=tools,
    )

    tool_calls = response.message.tool_calls  # è·å–æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨

    if tool_calls:
        # There may be multiple tool calls in the response
        for tool in tool_calls:
            # Ensure the function is available, and then call it
            if function_to_call := available_functions.get(tool.function.name):
                print('ğŸ”¹Calling function:', tool.function.name)
                print('ğŸ“¥Arguments:', tool.function.arguments)
                output = function_to_call(**tool.function.arguments)
                print('âœ…Function output:', output)

            else:
                print('âŒFunction', tool.function.name, 'not found')

    # Only needed to chat with the model using the tool call results
    if response.message.tool_calls:
        # Add the function response to messages for the model to use
        messages.append(response.message)
        messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})

        # Get final response from model with function outputs
        final_response = chat('llama3.1', messages=messages)
        print('Final responseğŸš€:', final_response.message.content)
        assistant_reply=final_response.message.content
    else:
        print('No tool calls returned from model')

    return assistant_reply


def agent_respond(messages: list):
    """æŒç»­å¯¹è¯æ¨¡å¼"""
    while True:
        user_input = input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
        if user_input.lower() in ["exit", "é€€å‡º"]:
            print("ğŸ”š ç»“æŸå¯¹è¯ã€‚")
            break

        messages.append({'role': 'user', 'content': user_input})
        assistant_reply = query_llm(messages)

        if assistant_reply:
            print(f'\nğŸ¤– AI: {assistant_reply}\n')
            messages.append({"role": "assistant", "content": assistant_reply})
        else:
            print("âš ï¸ AI æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„å“åº”ï¼Œè¯·é‡è¯•ã€‚")

if __name__ == "__main__":
    print("ğŸ¤– æ¬¢è¿ä½¿ç”¨ Ollama æ™ºèƒ½åŠ©æ‰‹ï¼è¾“å…¥ 'exit' é€€å‡ºå¯¹è¯ã€‚")


    # ç»´æŠ¤æ•´ä¸ªå¯¹è¯å†å²
    messages = [{'role': 'system', 'content': 'You are an AI assistant capable of calling external tools to complete user requests.'}]

    # è¿›å…¥å¯¹è¯ä¸»å¾ªç¯
    agent_respond(messages)