import ollama
import json
from ai_agent_omni import execute_ui
from prompt_manager import get_agent_prompt
from tools import tools, google_search, send_email, get_current_time

# ç”Ÿæˆæ ¼å¼åŒ–åçš„ JSON å·¥å…·åˆ—è¡¨
tools_json = json.dumps(tools, indent=2, ensure_ascii=False)

available_functions = {
  'google_search': google_search,
  'send_email': send_email,
  'get_current_time': get_current_time,
  'execute_ui':execute_ui
}


def query_llm(messages: list) -> str:
    """è°ƒç”¨ Ollama æœ¬åœ°APIè·å– LLM å¯¹æŒ‡å®šå¯¹è¯æ¶ˆæ¯çš„å›å¤ã€‚"""
    while True:
        response =ollama.chat(
            model='qwen2.5:14b',
            messages=messages,
            options={'temperature':0.6},
            tools=tools,
        )

        tool_calls = response.message.tool_calls  # è·å–æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨

        output = ''

        if tool_calls:
            # There may be multiple tool calls in the response
            for tool in tool_calls:
                # Ensure the function is available, and then call it
                if function_to_call := available_functions.get(tool.function.name):
                    print('ğŸ”¹Calling function:', tool.function.name)
                    print('ğŸ“¥Arguments:', tool.function.arguments)
                    output = function_to_call(**tool.function.arguments)
                    print('âœ…Function output:', output)

                else:
                    print('âŒFunction', tool.function.name, 'not found')

        # Only needed to chat with the model using the tool call results
        if response.message.tool_calls:
            # Add the function response to messages for the model to use
            messages.append(response.message)
            messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})

            # Get final response from model with function outputs
            #final_response = chat('qwen2.5:14b', messages=messages,tools=tools)
            #print('Final responseğŸš€:', final_response.message.content)
            #assistant_reply=final_response.message.content
        else:
            print('No tool calls returned from model')
            assistant_reply=response.message.content
            #print('Final responseğŸš€:', assistant_reply)
            break

    return assistant_reply


def agent_respond(messages: list):
    """æŒç»­å¯¹è¯æ¨¡å¼"""
    while True:
        user_input = input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
        if user_input.lower() in ["exit", "é€€å‡º"]:
            print("ğŸ”š ç»“æŸå¯¹è¯ã€‚")
            break

        messages.append({'role': 'user', 'content': user_input})
        assistant_reply = query_llm(messages)

        if assistant_reply:
            print(f'\nğŸ¤– AI: {assistant_reply}\n')
            messages.append({"role": "assistant", "content": assistant_reply})
        else:
            print("âš ï¸ AI æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„å“åº”ï¼Œè¯·é‡è¯•ã€‚")

if __name__ == "__main__":
    print("ğŸ¤– æ¬¢è¿ä½¿ç”¨ T-AIAgent æ™ºèƒ½åŠ©æ‰‹ï¼è¾“å…¥ 'exit' é€€å‡ºå¯¹è¯ã€‚")

    prompt = get_agent_prompt()

    # ç»´æŠ¤æ•´ä¸ªå¯¹è¯å†å²
    messages = [{'role': 'system', 'content': prompt}]

    # è¿›å…¥å¯¹è¯ä¸»å¾ªç¯
    agent_respond(messages)